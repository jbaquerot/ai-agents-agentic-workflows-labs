{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0158f-0816-4281-83d0-2ddc255f6b56",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af518f-7fab-4ab5-8580-d8f536384ee2",
   "metadata": {},
   "source": [
    "# **Build Interactive LLM Agents with Tools**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab83f9-1870-47e5-973d-8b4d8d0e4194",
   "metadata": {},
   "source": [
    "Estimated time needed: **15** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eefe26-bf6a-480c-9e27-3a6b41e087ad",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can dynamically interact with users. Using the LangChain framework, you’ll learn how to build an interactive agent that responds to user queries by selecting and executing the right function at the right time. This hands-on approach will help you understand how LLMs can be extended with real-world functionality, bridging natural language understanding with dynamic, tool-based actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5130d-8b3a-411b-ac10-19e5873a72df",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "- [Objectives](#Objectives)\n",
    "- [Setup](#Setup)\n",
    "    - [Installing Required Libraries](#Installing-Required-Libraries)\n",
    "    - [Importing Required Libraries](#Importing-Required-Libraries)\n",
    "- [Creating Custom Tools with LangChain](#Creating-Custom-Tools-with-LangChain)\n",
    "    - [Anatomy of a tool](#Anatomy-of-a-tool)\n",
    "    - [Key components](#Key-components)\n",
    "    - [Defining an add function](#Defining-an-add-function)\n",
    "    - [Add tools to the LLM](#Add-tools-to-the-LLM)\n",
    "    - [Create more Tools](#Create-more-tools)\n",
    "    - [Testing the functions](#Testing-the-functions)\n",
    "    - [Add new tools to LLM](#Add-new-tools-to-LLM)\n",
    "- [Interacting with the Model](#Interacting-with-the-Model)\n",
    "    - [Craft the user query](#Craft-the-user-query)\n",
    "    - [Invoke the model](#Invoke-the-model)\n",
    "    - [Parse tool calls](#Parse-tool-calls)\n",
    "    - [Invoke the tool](#Invoke-the-Tool)\n",
    "    - [Generate a final answer from chat history](#Generate-a-final-answer-from-chat-history)\n",
    "- [Building an Agent](#Building-an-Agent)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Exercises](#Exercises)\n",
    "    - [Exercise 1: Create a New Tool](#Exercise-1:-Create-a-new-tool)\n",
    "    - [Exercise 2: Tool Calling with an LLM](#Exercise-2:-Tool-calling-with-an-LLM)\n",
    "    - [Exercise 3: Create a tip calculating agent](#Exercise-3:-Create-a-tip-calculating-agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adbb4e-d6db-4751-86f9-6659c220c370",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Initialize a chat model for tool interactions\n",
    " - Define and bind custom tools to the LLM for expanded functionality\n",
    " - Use mapping dictionaries for dynamic function calls\n",
    " - Extract tool names and functions for precise function calls\n",
    " - Build agent classes that manage the entire tool-calling process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39de8c-7f53-471b-a5c9-8caafc2fcf3c",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ad0bd-35a7-4331-900a-c0f06ac3ff7d",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc72d6-b506-4c5e-b1c5-77a97a093322",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`langchain`](https://python.langchain.com/docs/introduction/) is the framework you will build the agent on.\n",
    "*   [`langchain-openai`](https://pypi.org/project/langchain-openai/) is a partner package of LangChain and integrates OpenAI LLMs to the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba168d-eae6-4dbe-8945-c2363931e88c",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120182de-506e-4c08-b6a0-810605b4efb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-mistralai 1.1.1 requires langchain-core<2.0.0,>=1.1.0, but you have langchain-core 0.3.83 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.3.83\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai===0.3.19) (2026.1.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.3.19 requires langchain-core<1.0.0,>=0.3.63, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-community 0.3.16 requires langchain-core<0.4.0,>=0.3.32, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain 0.3.25 requires langchain-core<1.0.0,>=0.3.58, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-ibm 0.3.10 requires langchain-core<0.4.0,>=0.3.39, but you have langchain-core 1.2.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain===0.3.25 | tail -n 1\n",
    "%pip install langchain-openai===0.3.19 | tail -n 1\n",
    "%pip install -qU langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.25 in ./.venv/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai==0.3.19 in ./.venv/lib/python3.12/site-packages (0.3.19)\n",
      "Collecting langchain-mistralai<1.0.0\n",
      "  Downloading langchain_mistralai-0.2.12-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain==0.3.25)\n",
      "  Using cached langchain_core-0.3.83-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./.venv/lib/python3.12/site-packages (from langchain==0.3.25) (0.3.11)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain==0.3.25) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain==0.3.25) (2.12.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain==0.3.25) (2.0.46)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain==0.3.25) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain==0.3.25) (6.0.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in ./.venv/lib/python3.12/site-packages (from langchain-openai==0.3.19) (1.68.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.12/site-packages (from langchain-openai==0.3.19) (0.12.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai<1.0.0) (0.22.2)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai<1.0.0) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai<1.0.0) (0.4.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai<1.0.0) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai<1.0.0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai<1.0.0) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai<1.0.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai<1.0.0) (0.16.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (25.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.19) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.19) (0.13.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.19) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.19) (4.67.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.3.25) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.3.25) (2.6.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.19) (2026.1.15)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (1.4.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (0.21.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai<1.0.0) (8.3.1)\n",
      "Downloading langchain_mistralai-0.2.12-py3-none-any.whl (16 kB)\n",
      "Using cached langchain_core-0.3.83-py3-none-any.whl (458 kB)\n",
      "Installing collected packages: langchain-core, langchain-mistralai\n",
      "\u001b[2K  Attempting uninstall: langchain-core\n",
      "\u001b[2K    Found existing installation: langchain-core 1.2.9\n",
      "\u001b[2K    Uninstalling langchain-core-1.2.9:\n",
      "\u001b[2K      Successfully uninstalled langchain-core-1.2.9\n",
      "\u001b[2K  Attempting uninstall: langchain-mistralai━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain-mistralai 1.1.1[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-mistralai-1.1.1:━\u001b[0m \u001b[32m0/2\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-mistralai-1.1.12\u001b[0m [langchain-core]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [langchain-mistralai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed langchain-core-0.3.83 langchain-mistralai-0.2.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.3.25 langchain-openai==0.3.19 'langchain-mistralai<1.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a181143-4919-44fd-a414-85ca1834d3f9",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "Recommendation:Import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa130c60-4515-4792-bc47-ac64f5e1f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010cbe9-7710-4949-be86-2e1bb634a7bc",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad60dc01-f3d3-4674-891c-fe44b3e3857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d624e5-391b-40f8-a46e-8fb3fa5938af",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "\n",
    "This lab uses LLMs provided by Watsonx.ai and OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges. \n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses `ChatOpenAI` and `ChatWatsonx` modules from `langchain`. Both configurations are shown below with instructions. **Replace all instances** of both modules with the completed modules below throughout the lab. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9134e645-c27e-4b32-a803-e8c565fa0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "mistral_llm = ChatMistralAI(\n",
    "    model=\"ministral-8b-2512\",\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b6550-ec74-4a18-b657-89c3a30c39ae",
   "metadata": {},
   "source": [
    "## Creating Custom Tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blocks of a tool, consider the following tool:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "### Key components\n",
    "\n",
    "You'll use the following key components\n",
    "\n",
    "**@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "**Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "**Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "**Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Is critical for tool selection by the LLM\n",
    "\n",
    "6. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edae93-e493-4990-ac22-bf369e0c95aa",
   "metadata": {},
   "source": [
    "### Defining an add function\n",
    "\n",
    "Now use this tool framework to create a custom tool that enables the LLM to perform basic addition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496ff457-4ee4-412e-9f5e-ae7db8c81142",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers together and return the sum.\n",
    "    \n",
    "    This tool performs basic addition operation on two integer values.\n",
    "    Use this when you need to calculate the sum of two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a (int): The first integer to be added\n",
    "        b (int): The second integer to be added\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of a and b\n",
    "        \n",
    "    Example:\n",
    "        >>> add(3, 5)\n",
    "        8\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute the addition operation\n",
    "        result = a + b\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors\n",
    "        raise ValueError(f\"Error performing addition: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c78dab-3494-4a6c-bc25-78badf868271",
   "metadata": {},
   "source": [
    "The decorator wraps the `add()` function in LangChain's predefined tool schema. See more about defining custom LangChain tools [here](https://python.langchain.com/docs/how_to/custom_tools/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b075448-27cb-4a2c-ab0b-cf58f1877555",
   "metadata": {},
   "source": [
    "### Add tools to the LLM\n",
    "\n",
    "Let's connect and bind the function to the chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757b00e8-fa57-4dcd-9cc5-3b9a8f138b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f668845-8a83-4e62-9f9e-75dfdb036d8b",
   "metadata": {},
   "source": [
    "Use the `bind_tools(tools)` method to connect a list of tools to the LLM for use. From now on, whenever the call is invoked, the model (with tools) will recognize and use the add tool whenever it needs to compute a sum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70dd7f-bbbf-4048-8e0d-772a22a2e936",
   "metadata": {},
   "source": [
    "### Create more tools\n",
    "\n",
    "Let's create some more basic arithmetic tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b723a05-abd9-41e0-97d3-c276c2a82a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract b from a and return the difference.\n",
    "    \n",
    "    This tool performs basic subtraction operation on two integer values.\n",
    "    Use this when you need to calculate the difference between two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a (int): The integer to subtract from (minuend)\n",
    "        b (int): The integer to subtract (subtrahend)\n",
    "\n",
    "    Returns:\n",
    "        int: The difference of a minus b\n",
    "        \n",
    "    Example:\n",
    "        >>> subtract(10, 3)\n",
    "        7\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute the subtraction operation\n",
    "        result = a - b\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors\n",
    "        raise ValueError(f\"Error performing subtraction: {str(e)}\")\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply a and b and return the product.\n",
    "    \n",
    "    This tool performs basic multiplication operation on two integer values.\n",
    "    Use this when you need to calculate the product of two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a (int): The first integer to multiply (multiplicand)\n",
    "        b (int): The second integer to multiply (multiplier)\n",
    "\n",
    "    Returns:\n",
    "        int: The product of a times b\n",
    "        \n",
    "    Example:\n",
    "        >>> multiply(4, 5)\n",
    "        20\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute the multiplication operation\n",
    "        result = a * b\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors\n",
    "        raise ValueError(f\"Error performing multiplication: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758603b-0fe0-4607-aab9-c4f7ddad9198",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "\n",
    "Let's setup a way to test your tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ca63f2-1b52-4881-b071-95a1f2be8e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_map = {\n",
    "    \"add\": add, \n",
    "    \"subtract\": subtract,\n",
    "    \"multiply\": multiply\n",
    "}\n",
    "\n",
    "input_ = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 2\n",
    "}\n",
    "\n",
    "tool_map[\"add\"].invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94371d46-8a5b-4ffe-a22c-a718723aa1d2",
   "metadata": {},
   "source": [
    "Using LangChain's built in `.invoke(inputs)` method, you can test each tool built with dynamic inputs.Test each tool with the preceding code block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805ce2e-fb15-43f7-a2db-a175a46aed38",
   "metadata": {},
   "source": [
    "### Add new tools to LLM\n",
    "\n",
    "Let's add all three tools to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f0ee64-36e9-46a1-a68d-2d64b9ceb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef994ac4-2c9d-499b-9b8f-89614420b7ad",
   "metadata": {},
   "source": [
    "You can the same method to bind tools to the LLM, enabling more arithmetic capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1198f-da13-46e2-9e70-ada0151c7129",
   "metadata": {},
   "source": [
    "## Interacting with the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3e1ea-f3a5-408f-bc3c-0407503d305d",
   "metadata": {},
   "source": [
    "### Craft the user query\n",
    "\n",
    "Now that you've setup an LLM with basic tool integrations, it's time to introduce user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736365bb-3874-4c00-9bde-6fbaa47d8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 + 2?\"\n",
    "chat_history = [HumanMessage(content=query)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccceb3b3-e906-467c-a48d-7379aecdcb80",
   "metadata": {},
   "source": [
    "First,setup the question (user query). Then,initialize a `chat_history` array that will contain the entire conversation between user and LLM. In this chat history, you insert the `query` in a `HumanMessage` wrapper that tells LangChain and the model: \"This message came from the user.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d609f3f-8925-4b80-87da-6b1cf9e61081",
   "metadata": {},
   "source": [
    "### Invoke the model\n",
    "\n",
    "Now let's run the model with the context (chat history) that contains the user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb504c35-b6be-46e5-a869-bbc311fabb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(chat_history)\n",
    "chat_history.append(response_1)\n",
    "\n",
    "print(type(response_1))\n",
    "#print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e1996-63b4-4615-b743-6f2f39bc5dd5",
   "metadata": {},
   "source": [
    "Using the `invoke(inputs)` method, you get a response from the model. You add the response to the chat history. The code block also prints out the type of the response which is the `AIMessage` class from LangChain. Uncomment the second print statement and read through the fields of the `AIMessage` response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c472f-c46e-41ee-aa42-e1f410448680",
   "metadata": {},
   "source": [
    "### Parse tool calls\n",
    "\n",
    "Now that you have the response from the model, you can parse the response for tool calling instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea25f20a-09c3-4c23-96d9-41a90f4087a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool name:\n",
      "add\n",
      "tool args:\n",
      "{'a': 3, 'b': 2}\n",
      "tool call ID:\n",
      "call_qVY5quELufvPfooJ8MSe9NPt\n"
     ]
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "\n",
    "tool_1_name = tool_calls_1[0][\"name\"]\n",
    "tool_1_args = tool_calls_1[0][\"args\"]\n",
    "tool_call_1_id = tool_calls_1[0][\"id\"]\n",
    "\n",
    "print(f'tool name:\\n{tool_1_name}')\n",
    "print(f'tool args:\\n{tool_1_args}')\n",
    "print(f'tool call ID:\\n{tool_call_1_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e9477-5607-4900-9967-63ae546a72b9",
   "metadata": {},
   "source": [
    "- Extracting the `name` from the first call gives the name of the tool to use.\n",
    "    - `add` in this case\n",
    "- Extracting the `args` gives the inputs to pass into the tool.\n",
    "    - `{a: 3, b: 2}` in this case\n",
    "- Extracting the `id` gives the unique identifier for the tool call\n",
    "    - The ID will be different each time, linking tool calls to their respective responses\n",
    "    - Crucial in differentiating calls to the same tool and parallel tool calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1845aa-215f-437d-bf46-7280a4de13d5",
   "metadata": {},
   "source": [
    "### Invoke the Tool\n",
    "\n",
    "Given the tool call details from the LLM, invoke the correct tool with the correct arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eaf2a40-9bc3-478f-be5e-ee69df81d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='5' tool_call_id='call_qVY5quELufvPfooJ8MSe9NPt'\n"
     ]
    }
   ],
   "source": [
    "tool_response = tool_map[tool_1_name].invoke(tool_1_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_1_id)\n",
    "\n",
    "print(tool_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4c61c-0008-47f9-8c43-053b8f8f8954",
   "metadata": {},
   "source": [
    "Use the `tool_map`, passing in the tool name and parameters to get a response. Then wrap that response in a `ToolMessage` object from LangChain along with the tool call ID. This action allows the model and LangChain to better process tool responses and overall conversation between user and model and tool. Feel free to uncomment the print statement to see what the `tool_message` looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4751f77-aae7-41e4-a20c-3a3c3b6b4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append(tool_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb63504-9948-43a8-b56e-e1f28266144a",
   "metadata": {},
   "source": [
    "Next, append the `tool_message` to the `chat_history` so the model preserves context and sees prior conversation for a better conversing experience. Now the chat history contains a `HumanMessage` (initial user query), an `AIMessage` (the response from the model), and a `ToolMessage` (the output of the tool).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b8a80-3e7e-4a3f-8be7-f07bf5a43894",
   "metadata": {},
   "source": [
    "### Generate a final answer from chat history\n",
    "\n",
    "As a final step, pass the entire `chat_history` into the LLM one more time to get a final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48877d2a-9b75-44b5-93c2-02d2b9b83174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "3 + 2 equals 5.\n"
     ]
    }
   ],
   "source": [
    "answer = llm_with_tools.invoke(chat_history)\n",
    "print(type(answer))\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092ed0c-16b0-46c4-987c-da6c21d6b38b",
   "metadata": {},
   "source": [
    "Printing the `answer.content` (content field of the `AIMessage` object) gives the final result of the LLM for the user query. You have finished a complete interaction between the user and model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77de361-db77-48a7-adf8-863faab9deee",
   "metadata": {},
   "source": [
    "## Building an Agent\n",
    "\n",
    "You can wrap all the prior functionality in a unified Agent class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c96884-5320-4186-8d4f-565715bb0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallingAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tools = llm.bind_tools(tools)\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        # Step 1: Initial user message\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "        # Step 2: LLM chooses tool\n",
    "        response = self.llm_with_tools.invoke(chat_history)\n",
    "        if not response.tool_calls:\n",
    "            return response.contet # Direct response, no tool needed\n",
    "        # Step 3: Handle first tool call\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_call_id = tool_call[\"id\"]\n",
    "\n",
    "        # Step 4: Call tool manually\n",
    "        tool_result = self.tool_map[tool_name].invoke(tool_args)\n",
    "\n",
    "        # Step 5: Send result back to LLM\n",
    "        tool_message = ToolMessage(content=str(tool_result), tool_call_id=tool_call_id)\n",
    "        chat_history.extend([response, tool_message])\n",
    "\n",
    "        # Step 6: Final LLM result\n",
    "        final_response = self.llm_with_tools.invoke(chat_history)\n",
    "        return final_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c4867-f949-4100-81ab-2e6396ec274c",
   "metadata": {},
   "source": [
    "This agent does the exact same process as above except the interaction with the model handling is all contained within the `run()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e465dd10-dfc9-4f1b-8a1b-f9230d64614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One plus two equals three.\n",
      "The result of \\(1 - 2\\) is \\(-1\\).\n",
      "Three times two is 6.\n"
     ]
    }
   ],
   "source": [
    "my_agent = ToolCallingAgent(llm)\n",
    "\n",
    "print(my_agent.run(\"one plus 2\"))\n",
    "\n",
    "print(my_agent.run(\"one - 2\"))\n",
    "\n",
    "print(my_agent.run(\"three times two\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f0f88-bbc8-4255-8cbb-b1ef24d609b6",
   "metadata": {},
   "source": [
    "Here are three examples of the agent in use. These agents are dynamic and can handle many different types and formats of data as input. This capability is a major benefit of AI agents as the input data doesn't need to be normalized or formatted a certain way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7b17a-dbc5-45eb-b320-5ec81ee6d531",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now completed this short introduction to building interactive tool calling agents. Now you can:\n",
    "- Structure user interactions and setup chat models for real-time, context-aware conversations\n",
    "- Extracte tool names and arguments to precisely match user intent\n",
    "- Parse complex tool instructions, including handling multiple tool calls\n",
    "- Build and refine an agent class to automate the entire tool-calling process\n",
    "- Dempnstrate how these components work together to transform LLMs from passive responders to intelligent agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0265bdb9-a834-48df-b6e0-442c0f098a91",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbe674-da10-4f08-a9a0-3a9a435ed0a4",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a new tool\n",
    "\n",
    "Use the example tool format provided in the notebook to create a new tool named `calculate_tip` that takes a `total_bill and tip_percent`, and returns the tip amount. </br>\n",
    "Define and invoke the tool with sample inputs like `total_bill=120`, `tip_percent=15`. </br>\n",
    "Create a `tool_map` with the `calculate_tip` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39ef1c39-47bf-45ac-aedd-dffa418199e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 1\n",
    "@tool\n",
    "def calculate_tip(total_bill: float, tip_percent: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the tip amount based on total bill and tip percentage.\n",
    "    \n",
    "    This tool calculates how much tip to leave based on the total bill amount\n",
    "    and the desired tip percentage.\n",
    "    Use this when you need to calculate the tip for a bill.\n",
    "    \n",
    "    Args:\n",
    "        total_bill (float): The total bill amount before tip\n",
    "        tip_percent (float): The tip percentage (e.g., 15 for 15%)\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated tip amount\n",
    "        \n",
    "    Example:\n",
    "        >>> calculate_tip(100, 15)\n",
    "        15.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tip_amount = total_bill * tip_percent / 100\n",
    "        return tip_amount\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error calculating tip: {str(e)}\")\n",
    "\n",
    "# Test the tool\n",
    "inputs = {\n",
    "    \"total_bill\": 120,\n",
    "    \"tip_percent\": 15\n",
    "}\n",
    "print(calculate_tip.invoke(inputs))\n",
    "\n",
    "# Create tool_map with calculate_tip\n",
    "tip_tool_map = {\n",
    "    \"calculate_tip\": calculate_tip\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326a8e6-36c1-43ce-9be0-c58fd18ee742",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def calculate_tip(total_bill: int, tip_percent: int) -> int:\n",
    "    \"\"\"Calculate tip\"\"\"\n",
    "    return total_bill * tip_percent * 0.01\n",
    "\n",
    "inputs = {\n",
    "    \"total_bill\": 120,\n",
    "    \"tip_percent\": 15\n",
    "}\n",
    "calculate_tip.invoke(inputs)\n",
    "\n",
    "\n",
    "tool_map = {\n",
    "    \"calculate_tip\": calculate_tip\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5cc35-d6b3-4cd2-bfea-4470feb706d5",
   "metadata": {},
   "source": [
    "### Exercise 2: Tool calling with an LLM\n",
    "\n",
    "Simulate a user query like \"How much should I tip on $60 at 20%?\". </br>\n",
    "Bind the tool to the predefined `llm` and prompt the LLM with the query above. Then parse the LLM response for the tool calling details and invoke the tool accordingly. Finally, take the entire chat history and prompt the LLM for a final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new tool to the llm_with_tools\n",
    "tools.append(calculate_tip)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "tool_map[\"calculate_tip\"] = calculate_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': StructuredTool(name='add', description='Add two integers together and return the sum.\\n\\nThis tool performs basic addition operation on two integer values.\\nUse this when you need to calculate the sum of two numbers.\\n\\nArgs:\\n    a (int): The first integer to be added\\n    b (int): The second integer to be added\\n\\nReturns:\\n    int: The sum of a and b\\n\\nExample:\\n    >>> add(3, 5)\\n    8', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x1126ed6c0>),\n",
       " 'subtract': StructuredTool(name='subtract', description='Subtract b from a and return the difference.\\n\\nThis tool performs basic subtraction operation on two integer values.\\nUse this when you need to calculate the difference between two numbers.\\n\\nArgs:\\n    a (int): The integer to subtract from (minuend)\\n    b (int): The integer to subtract (subtrahend)\\n\\nReturns:\\n    int: The difference of a minus b\\n\\nExample:\\n    >>> subtract(10, 3)\\n    7', args_schema=<class 'langchain_core.utils.pydantic.subtract'>, func=<function subtract at 0x1126ed940>),\n",
       " 'multiply': StructuredTool(name='multiply', description='Multiply a and b and return the product.\\n\\nThis tool performs basic multiplication operation on two integer values.\\nUse this when you need to calculate the product of two numbers.\\n\\nArgs:\\n    a (int): The first integer to multiply (multiplicand)\\n    b (int): The second integer to multiply (multiplier)\\n\\nReturns:\\n    int: The product of a times b\\n\\nExample:\\n    >>> multiply(4, 5)\\n    20', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x1126eda80>),\n",
       " 'calculate_tip': StructuredTool(name='calculate_tip', description='Calculate the tip amount based on total bill and tip percentage.\\n\\nThis tool calculates how much tip to leave based on the total bill amount\\nand the desired tip percentage.\\nUse this when you need to calculate the tip for a bill.\\n\\nArgs:\\n    total_bill (float): The total bill amount before tip\\n    tip_percent (float): The tip percentage (e.g., 15 for 15%)\\n\\nReturns:\\n    float: The calculated tip amount\\n\\nExample:\\n    >>> calculate_tip(100, 15)\\n    15.0', args_schema=<class 'langchain_core.utils.pydantic.calculate_tip'>, func=<function calculate_tip at 0x1126ee2a0>)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9542d0b-1bc9-41f0-a2b5-cf165278357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "tool name:\n",
      "calculate_tip\n",
      "tool args:\n",
      "{'total_bill': 60, 'tip_percent': 20}\n",
      "tool call ID:\n",
      "call_X19yyZqsbKWxvo4NiW83s0PF\n",
      "content='12.0' tool_call_id='call_X19yyZqsbKWxvo4NiW83s0PF'\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "You should tip $12 on a $60 bill at 20%.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 2\n",
    "# Craft the user query\n",
    "query = \"How much should I tip on $60 at 20%?\"\n",
    "chat_history = [HumanMessage(content=query)]\n",
    "# Invoke the model\n",
    "response_1 = llm_with_tools.invoke(chat_history)\n",
    "chat_history.append(response_1)\n",
    "print(type(response_1))\n",
    "\n",
    "# Parse tool calls\n",
    "tool_calls_1 = response_1.tool_calls\n",
    "\n",
    "tool_1_name = tool_calls_1[0][\"name\"]\n",
    "tool_1_args = tool_calls_1[0][\"args\"]\n",
    "tool_call_1_id = tool_calls_1[0][\"id\"]\n",
    "\n",
    "print(f'tool name:\\n{tool_1_name}')\n",
    "print(f'tool args:\\n{tool_1_args}')\n",
    "print(f'tool call ID:\\n{tool_call_1_id}')\n",
    "\n",
    "# Invoke the Tool\n",
    "tool_response = tool_map[tool_1_name].invoke(tool_1_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_1_id)\n",
    "\n",
    "print(tool_message)\n",
    "\n",
    "# Update chat history\n",
    "chat_history.append(tool_message)\n",
    "\n",
    "# Generate a final answer for chat history\n",
    "answer = llm_with_tools.invoke(chat_history)\n",
    "print(type(answer))\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae173e-672e-4b44-aac3-1b7630c33ca0",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "tool_calls = response.tool_calls\n",
    "tool_name = tool_calls[0][\"name\"]\n",
    "tool_args = tool_calls[0][\"args\"]\n",
    "tool_call_id = tool_calls[0][\"id\"]\n",
    "\n",
    "tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "\n",
    "chat_history.extend([response, tool_message])\n",
    "\n",
    "result = llm_with_tool.invoke(chat_history)\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5afd1-8d05-4cd6-8ca6-29bf2faa8fef",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a tip calculating agent\n",
    "\n",
    "Create an agent to automate the entire process you previously completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallingAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tools = llm.bind_tools(tools)\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        # Step 1: Initial user message\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "        # Step 2: LLM chooses tool\n",
    "        response = self.llm_with_tools.invoke(chat_history)\n",
    "        if not response.tool_calls:\n",
    "            return response.contet # Direct response, no tool needed\n",
    "        # Step 3: Handle first tool call\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_call_id = tool_call[\"id\"]\n",
    "\n",
    "        # Step 4: Call tool manually\n",
    "        tool_result = self.tool_map[tool_name].invoke(tool_args)\n",
    "\n",
    "        # Step 5: Send result back to LLM\n",
    "        tool_message = ToolMessage(content=str(tool_result), tool_call_id=tool_call_id)\n",
    "        chat_history.extend([response, tool_message])\n",
    "\n",
    "        # Step 6: Final LLM result\n",
    "        final_response = self.llm_with_tools.invoke(chat_history)\n",
    "        return final_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a4d612b-6a60-4b2c-9fb2-0c45c449db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should tip $12 on a $60 bill at 20%.\n",
      "One plus two equals three.\n",
      "The result of subtracting 2 from 1 is -1.\n",
      "Three times two equals 6.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 3\n",
    "query = \"How much should I tip on $60 at 20%?\"\n",
    "my_agent2 = ToolCallingAgent(llm)\n",
    "\n",
    "print(my_agent2.run(query))\n",
    "\n",
    "print(my_agent2.run(\"one plus 2\"))\n",
    "\n",
    "print(my_agent2.run(\"one - 2\"))\n",
    "\n",
    "print(my_agent2.run(\"three times two\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48a423-1bc8-495a-8411-552942c4a20c",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "class TipAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "        response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "        tool_calls = response.tool_calls\n",
    "        tool_name = tool_calls[0][\"name\"]\n",
    "        tool_args = tool_calls[0][\"args\"]\n",
    "        tool_call_id = tool_calls[0][\"id\"]\n",
    "        \n",
    "        tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "        tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "        \n",
    "        chat_history.extend([response, tool_message])\n",
    "        \n",
    "        return llm_with_tool.invoke(chat_history).content\n",
    "\n",
    "agent = TipAgent(llm)\n",
    "agent.run(\"How much should I tip on $60 at 20%?\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a09d85-2eea-4b26-9134-7370c33324a0",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c567e4-ed51-41fa-b0a9-fbb9bac1a38e",
   "metadata": {},
   "source": [
    "[Joshua Zhou](https://author.skills.network/instructors/joshua_zhou)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1df22f-e62a-492a-8151-0fc9e68dd4cd",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a5468-45e2-4ed6-8772-2456bb7e87a3",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana)</br>\n",
    "[Karan Goswami](https://author.skills.network/instructors/karan_goswami)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac53bb7-afc3-41ef-a472-cfb7b8151b14",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecc6c3-ec11-4ddc-af65-31ee0197dbb5",
   "metadata": {},
   "source": [
    "<!-- ## Changelog\n",
    "\n",
    "| Date | Version | Changed by | Change Description |\n",
    "|------|--------|--------|---------|\n",
    "| 2024-06-06 | 0.1 |  P. Kravitz | ID review and edit. No code edits.Updated the copyright statement. Change log added. Instructional edits only for IBM style. Second person, accessibility, and other minor grammar edits.| -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "prev_pub_hash": "3ed25488039fa4c590a60c6be98ebd97e5fa32f9525446191ec94fcf70e9fc62"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
